# -*- coding: utf-8 -*-
"""Projek_Akhir_ML_Terapan_Sistem_Rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLgXxE5og5jQ3qwei-q_Aix_2_xsVRwS

# Import Library
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf_lib
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

"""# Load Dataset"""

from google.colab import drive
drive.mount('/content/drive')

chess = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Projek Akhir Machine Learning Terapan /datasets/games.csv")

chess.head()

# Visualisasi distribusi rating
plt.figure(figsize=(10, 6))
sns.histplot(chess['white_rating'], bins=50, kde=True, color='blue', label='White Rating')
sns.histplot(chess['black_rating'], bins=50, kde=True, color='red', label='Black Rating')
plt.title('Distribusi Rating Pemain Putih dan Hitam')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.legend()
plt.show()

"""# EDA"""

chess.info()

print("Banyaknya permainan: ", len(chess.id.unique()))
print("Banyaknya pemain putih: ", len(chess.white_id.unique()))
print("Banyaknya pemain hitam: ", len(chess.black_id.unique()))
print("Banyaknya opening: ", len(chess.opening_name.unique()))

# Mengecek statistik data
chess.describe()

# Mengecek apakah ada missing values
chess.isnull().sum()

# mengecek apakah ada yang duplicate
chess['id'].duplicated().sum()

# Menemukan baris yang duplikat berdasarkan kolom 'id'
duplikat = chess[chess.duplicated(subset=['id'])]
print(duplikat)

# Mennghapus data permainan yang sama
chess = chess.drop_duplicates(subset=['id'])
chess.info()

"""# Data Preprocessing

## Filter by rating range
"""

chess_rating = (
    ((chess['white_rating'] >= 1500) & (chess['black_rating'] <= 2200)) &
    ((chess['black_rating'] >= 1500) & (chess['white_rating'] <= 2200))
)
chess_data = chess[chess_rating].copy()
chess_data.head()

# Visualisasi distribusi rating setelah filter
plt.figure(figsize=(10, 6))
sns.histplot(chess_data['white_rating'], bins=50, kde=True, color='blue', label='White Rating')
sns.histplot(chess_data['black_rating'], bins=50, kde=True, color='red', label='Black Rating')
plt.title('Distribusi Rating Pemain Putih dan Hitam (Setelah Filter)')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.legend()
plt.show()

"""## Mengubah opening name menjadi opening archetype"""

chess_data = (
    chess_data.assign(
        opening_archetype=chess_data.opening_name.map(
            lambda n: n.split(":")[0].split("|")[0].split("#")[0].strip()
        ),
        opening_moves=chess_data.apply(lambda srs: srs['moves'].split(" ")[:srs['opening_ply']],
                                  axis=1)
    )
)

"""Melihat peermainan yang dimainkan oleh satu pemain setidaknya minimal 2 kali"""

chess_played = pd.concat([chess_data['white_id'], chess_data['black_id']]).value_counts()
chess_played.reset_index(drop=True).plot.line(figsize=(16, 8), fontsize=18)

n_ge_2 = len(chess_played[chess_played > 1])
print(str(n_ge_2) + " pemain yang bermain catur setidaknya dua kali.")

import matplotlib.pyplot as plt
plt.axvline(n_ge_2, color='green')

len(chess_played)

chess_played[chess_played > 1].sum()

opening_used = (
    pd.concat([
        chess_data.groupby('white_id')['opening_archetype'].value_counts(),
        chess_data.groupby('black_id')['opening_archetype'].value_counts()
    ])
    .rename_axis(index=['player_id', 'opening_archetype'])  # Ganti nama indeks dengan list-like
    .reset_index(name='times_used')  # Reset indeks dan beri nama kolom hasil value_counts
    .groupby(['player_id', 'opening_archetype'])['times_used'].sum()  # Jumlahkan times_used
    .reset_index()  # Reset indeks untuk hasil akhir
)

opening_used.head(10)

# Menemukan opening_archetype dengan times_used terbanyak untuk setiap player_id
most_used_openings = opening_used.loc[opening_used.groupby('player_id')['times_used'].idxmax()]

# Menampilkan hasil
most_used_openings.head(10)

opening_used.info()

most_used_openings.info()

result = (
    opening_used
    .reset_index()  # Reset indeks untuk memudahkan manipulasi
    .groupby('opening_archetype')  # Kelompokkan berdasarkan opening_archetype
    .times_used
    .sum()  # Jumlahkan times_used
    .sort_values(ascending=False)  # Urutkan secara menurun
    .to_frame()  # Ubah menjadi DataFrame
    .pipe(lambda df: df.assign(times_used=df.times_used / df.times_used.sum()))  # Hitung proporsi
    .squeeze()  # Ubah menjadi Series
    .head(10)  # Ambil 10 teratas
)

print(result)

# Visualisasi distribusi opening archetype
plt.figure(figsize=(12, 8))
chess_data['opening_archetype'].value_counts().head(20).plot(kind='bar', color='skyblue')
plt.title('Top 20 Opening Archetype')
plt.xlabel('Opening Archetype')
plt.ylabel('Frekuensi')
plt.show()

"""## Feature Selection"""

# Mengonversi data series ‘player_id’ menjadi dalam bentuk list
player_id = opening_used['player_id'].tolist()

# Mengonversi data series ‘opening_archetype’ menjadi dalam bentuk list
opening_archetype = opening_used['opening_archetype'].tolist()

# Mengonversi data series ‘times_used’ menjadi dalam bentuk list
times_used = opening_used['times_used'].tolist()


print(len(player_id))
print(len(opening_archetype))
print(len(times_used))

# Membuat dictionary untuk data ‘resto_id’, ‘resto_name’, dan ‘cuisine’
chess_new = pd.DataFrame({
    'id': player_id,
    'opening_archetype': opening_archetype,
    'times_used': times_used
})
chess_new

"""# Model Development with Content Based Filtering"""

data = chess_new
data.sample(5)

"""## TF-ID Vectorizer"""

# Inisialisasi
tf = TfidfVectorizer()

tf.fit(data['opening_archetype'].unique())

tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['opening_archetype'].unique())

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama resto

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.opening_archetype.unique()
).sample(22, axis=1).sample(10, axis=0)

"""## Cosine Similarity"""

# Compute cosine similarity
# Menghapus duplikasi dari 'opening_archetype' untuk memastikan keunikan
unique_openings = data['opening_archetype'].drop_duplicates()

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=unique_openings, columns=unique_openings)
cosine_sim_df

cosine_sim_df = pd.DataFrame(cosine_sim, index=unique_openings, columns=unique_openings)
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Visualisasi matriks cosine similarity
plt.figure(figsize=(12, 8))
sns.heatmap(cosine_sim_df.iloc[:10, :10], cmap='viridis', annot=True)
plt.title('Matriks Cosine Similarity (10x10)')
plt.show()

def opening_recommendations(player_id, k=5):

    # Find all openings used by the player
    player_preferences = data[data['id'] == player_id]

    if player_preferences.empty:
        print(f"No player data found for '{player_id}'.")
        return []

    used_openings = player_preferences['opening_archetype'].unique().tolist()

    # Daftar opening yang belum digunakan oleh pengguna
    opening_not_used = [opening for opening in data['opening_archetype'].unique() if opening not in used_openings]

    # Gabungkan opening yang sudah dan belum digunakan tanpa duplikasi
    all_openings = list(set(used_openings + opening_not_used))

    recommendations = []

    for opening in all_openings:
        # Calculate similarity using cosine similarity matrix
        similarity_scores = []
        for player_opening in used_openings:
            if player_opening in cosine_sim_df.index and opening in cosine_sim_df.columns:
                similarity_scores.append(cosine_sim_df.loc[player_opening, opening])

        if similarity_scores:
            avg_similarity = np.mean(similarity_scores)
            recommendations.append((opening, avg_similarity))

    recommendations.sort(key=lambda x: x[1], reverse=True)

    # Return top k recommendations (including used ones)
    return [rec[0] for rec in recommendations[:k]]

data[data.id.eq('yesman81')]

# Example usage
player_id = 'yesman81'
print(f"\nRekomendasi opening catur untuk user {player_id}:")
recommended_openings = opening_recommendations(player_id)

for opening in recommended_openings:
    print(opening)

"""## Evaluasi Precision untuk Content Based Filtering"""

def precision_at_k(recommended, relevant, k=5):
    """
    Calculate Precision@K.

    :param recommended: List of recommended items.
    :param relevant: List of relevant items.
    :param k: Number of recommendations to consider.
    :return: Precision@K score.
    """
    recommended_at_k = recommended[:k]  # Ambil top-K rekomendasi
    relevant_set = set(relevant)  # Ubah ke set untuk pencarian yang lebih cepat
    hits = sum(1 for item in recommended_at_k if item in relevant_set)  # Hitung item yang relevan
    return hits / k  # Hitung Precision@K

# Contoh penggunaan
player_id = 'yesman81'  # Ganti dengan ID pemain yang valid
# k = 5  # Jumlah rekomendasi

# Dapatkan rekomendasi
recommended_openings = opening_recommendations(player_id)

# Tentukan opening yang relevan (dalam hal ini, opening yang sudah digunakan oleh pemain)
player_preferences = data[data['id'] == player_id]
relevant_openings = player_preferences['opening_archetype'].unique().tolist()

# Hitung Precision@K
precision = precision_at_k(recommended_openings, relevant_openings)
print(f"Precision: {precision:.4f}")

"""# Model Development with Collaborative Filtering

## Encode User dan Opening
"""

# Encode user_id
user_ids = data['id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Encode opening_archetype
opening_ids = data['opening_archetype'].unique().tolist()
opening_to_opening_encoded = {x: i for i, x in enumerate(opening_ids)}
opening_encoded_to_opening = {i: x for i, x in enumerate(opening_ids)}

# Mapping ke dataset
data['user'] = data['id'].map(user_to_user_encoded)
data['opening'] = data['opening_archetype'].map(opening_to_opening_encoded)

"""## Normalisasi Rating"""

# Normalisasi times_used ke skala 0-1
min_rating = min(data['times_used'])
max_rating = max(data['times_used'])
data['rating'] = data['times_used'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

"""## Membagi Data untuk Training dan Validasi"""

# Acak dataset
data = data.sample(frac=1, random_state=42)

# Membagi data
x = data[['user', 'opening']].values
y = data['rating'].values

train_indices = int(0.8 * len(data))
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

# Tampilkan hasil
print("Data Training:")
print(x_train, y_train)

print("\nData Validasi:")
print(x_val, y_val)

"""## Proses Training"""

class ChessRecommenderNet(tf_lib.keras.Model):
    def __init__(self, num_users, num_openings, embedding_size, **kwargs):
        super(ChessRecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_openings = num_openings
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.opening_embedding = layers.Embedding(
            num_openings,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.opening_bias = layers.Embedding(num_openings, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        opening_vector = self.opening_embedding(inputs[:, 1])
        opening_bias = self.opening_bias(inputs[:, 1])

        dot_user_opening = tf_lib.tensordot(user_vector, opening_vector, 2)
        x = dot_user_opening + user_bias + opening_bias

        return tf_lib.nn.sigmoid(x)

# Inisialisasi model
num_users = len(user_to_user_encoded)
num_openings = len(opening_to_opening_encoded)
model = ChessRecommenderNet(num_users, num_openings, 50)

# Compile model
model.compile(
    loss=tf_lib.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf_lib.keras.metrics.RootMeanSquaredError()]
)

# Training model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=100,
    validation_data=(x_val, y_val)
)

# Plot RMSE
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Model Metrics')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend()
plt.show()

def get_opening_recommendations(user_id, top_k=5):
    # Daftar opening yang sudah digunakan oleh pengguna
    opening_used_by_user = data[data['user'] == user_to_user_encoded[user_id]]['opening']

    # Daftar opening yang belum digunakan oleh pengguna
    opening_not_used = [opening for opening in range(num_openings) if opening not in opening_used_by_user]

    # Gabungkan opening yang sudah digunakan dan yang belum digunakan tanpa duplikasi
    all_openings = list(set(opening_used_by_user.tolist() + opening_not_used))

    # Buat input untuk model
    user_encoder = user_to_user_encoded[user_id]
    user_opening_array = np.array([[user_encoder, opening] for opening in all_openings])

    # Cek jika user_opening_array kosong
    if user_opening_array.size == 0:
        print(f"No openings available for user {user_id}.")
        return

    # Prediksi rating untuk semua opening yang relevan
    ratings = model.predict(user_opening_array).flatten()

    # Ambil indeks top-k dari rating tertinggi
    top_ratings_indices = ratings.argsort()[-top_k:][::-1]

    # Mendapatkan rekomendasi berdasarkan top rating
    recommended_opening_ids = [all_openings[i] for i in top_ratings_indices]
    recommended_openings = [opening_encoded_to_opening[opening_id] for opening_id in recommended_opening_ids]

    print(f"Rekomendasi opening catur untuk user {user_id}:")
    for opening in recommended_openings:
        print(opening)

# Contoh penggunaan
get_opening_recommendations('yesman81')

"""## Evaluasi RMSE untuk Collaborative Filtering



"""

# Evaluasi model pada data validasi
val_loss, val_rmse = model.evaluate(x_val, y_val, batch_size=32)
loss, rmse = model.evaluate(x_train, y_train, batch_size=32)

print(f"Validation Loss: {round(val_loss, 4)}")
print(f"Validation RMSE: {round(val_rmse, 4)}")
print(f"Loss: {round(loss, 4)}")
print(f"RMSE: {round(rmse, 4)}")